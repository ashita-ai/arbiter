# Arbiter Project Plan

**Version:** 1.0
**Timeline:** 7 months to v1.0
**Current Status:** Phase 2 Complete, Phase 2.5 Starting
**Last Updated:** 2025-11-12

---

## Overview

This document provides the complete multi-milestone roadmap for Arbiter, from foundation through v1.0 release.

**Goal:** Build production-grade LLM evaluation infrastructure with simple API, complete observability, and provider-agnostic design.

**Key Metrics:**
- 8+ evaluators by Month 4
- >80% test coverage
- <200ms p95 latency
- 100+ GitHub stars by Month 3

---

## Timeline Summary

| Phase | Duration | Status | Completion |
|-------|----------|--------|------------|
| Phase 1: Foundation | 2 weeks | ‚úÖ Done | 100% |
| Phase 2: Core Engine | 1 week | ‚úÖ Done | 100% |
| **Phase 2.5: Critical Gaps** | **2-3 weeks** | **üöß Current** | **0%** |
| Phase 3: Semantic Comparison | 2 weeks | ‚è≥ Next | 0% |
| Phase 4: Storage & Batch | 2 weeks | ‚è≥ Planned | 0% |
| Phase 5: Core Evaluators | 6 weeks | ‚è≥ Planned | 0% |
| Phase 6: Quality Assurance | 2-3 weeks | ‚è≥ Planned | 0% |
| Phase 7: Streaming | 1 week | ‚è≥ Future | 0% |
| Phase 8: Testing & Docs | 2 weeks | ‚è≥ Future | 0% |
| Phase 9: Polish & Release | 2 weeks | ‚è≥ Future | 0% |

**Total:** ~6-7 months to v1.0

---

## Milestone Details

## Phase 1: Foundation ‚úÖ COMPLETE

**Duration:** 2 weeks (Nov 1-14, 2025)
**Status:** ‚úÖ 100% Complete
**Goal:** Project setup and core infrastructure

### Objectives
1. Set up project structure
2. Configure dependencies and tooling
3. Establish development workflow
4. Create initial documentation

### Deliverables

#### ‚úÖ Project Setup
- [x] Create project structure (arbiter/, tests/, examples/)
- [x] Initialize git repository
- [x] Configure pyproject.toml with dependencies
- [x] Create package structure (__init__.py files)
- [x] Add supporting files (README, LICENSE, .gitignore)
- [x] Initial commit

#### ‚úÖ Core Infrastructure
- [x] Middleware system (from Sifaka)
  - LoggingMiddleware
  - MetricsMiddleware
  - CachingMiddleware
  - RateLimitingMiddleware
- [x] LLM client infrastructure
  - core/llm_client.py (provider-agnostic)
  - core/llm_client_pool.py (connection pooling)
- [x] Retry logic with exponential backoff
- [x] Core models (EvaluationResult, Score, Metric, LLMInteraction)
- [x] Exception hierarchy (8 exception types)
- [x] Type definitions and protocols

### Success Criteria
- ‚úÖ Project builds successfully
- ‚úÖ All type checks pass (mypy strict)
- ‚úÖ Git repository initialized
- ‚úÖ Dependencies installed

### Lessons Learned
- Strong foundation enables rapid development
- PydanticAI integration smooth
- Early type safety prevents later issues

---

## Phase 2: Core Evaluation Engine ‚úÖ COMPLETE

**Duration:** 1 week (Nov 15-21, 2025)
**Status:** ‚úÖ 100% Complete + Critical Fixes
**Goal:** Working evaluation with SemanticEvaluator

### Objectives
1. Implement evaluation flow
2. Create PydanticAI evaluator base class
3. Build SemanticEvaluator
4. Design public API
5. Automatic LLM interaction tracking

### Deliverables

#### ‚úÖ Evaluation Engine
- [x] BaseEvaluator protocol (core/interfaces.py)
- [x] BasePydanticEvaluator template method class
  - _get_system_prompt()
  - _get_user_prompt()
  - _get_response_type()
  - _compute_score()
- [x] Automatic LLM interaction tracking
- [x] Score aggregation and confidence calculation

#### ‚úÖ SemanticEvaluator
- [x] Implementation in evaluators/semantic.py
- [x] SemanticResponse model (score, confidence, explanation, similarities/differences)
- [x] Handles 3 modes: with reference, criteria only, standalone

#### ‚úÖ Public API
- [x] evaluate() function in api.py
- [x] Automatic client management
- [x] Input validation
- [x] Error handling
- [x] Middleware integration

#### ‚úÖ Critical Fixes (completed during Phase 2)
- [x] Token counting fixed (extract from PydanticAI)
- [x] Middleware integrated into evaluate()
- [x] Input validation added

### Success Criteria
- ‚úÖ Can evaluate text with simple API
- ‚úÖ SemanticEvaluator returns structured scores
- ‚úÖ Complete LLM interaction tracking works
- ‚úÖ Example code demonstrates usage
- ‚úÖ All tests pass

### Key Achievements
- **Automatic interaction tracking** - Novel feature, well-implemented
- **Simple 3-line API** - Competitive with best-in-class
- **Template method pattern** - Forces consistency

### Gaps Identified
- Only one evaluator (semantic)
- No custom criteria support
- No comparison mode
- Limited documentation

---

## Phase 2.5: Fill Critical Gaps üöß CURRENT

**Duration:** 2-3 weeks (Nov 22 - Dec 12, 2025)
**Status:** üöß 0% Complete (Starting)
**Priority:** üî¥ CRITICAL
**Goal:** Make Phase 2 production-ready with key missing features

### Objectives
1. Add CustomCriteriaEvaluator (highest ROI)
2. Add PairwiseComparisonEvaluator (A/B testing)
3. Improve multi-evaluator error handling
4. Expand documentation significantly

### Deliverables

#### üî¥ CustomCriteriaEvaluator (2-3 days)
**Priority:** HIGHEST
**Why:** Multiplies framework usefulness immediately

**Features:**
- Single criteria evaluation
- Multi-criteria evaluation (dict input)
- Flexible prompt templates
- Domain-specific criteria support

**Example:**
```python
result = await evaluate(
    output="Medical advice...",
    criteria="Medical accuracy, HIPAA compliance, appropriate tone",
    evaluators=["custom_criteria"],
    model="gpt-4o"
)
```

**Deliverables:**
- [ ] CustomCriteriaEvaluator class
- [ ] CustomCriteriaResponse model
- [ ] Single criteria mode
- [ ] Multi-criteria mode (returns multiple scores)
- [ ] Tests (>80% coverage)
- [ ] Example code
- [ ] Documentation

#### üî¥ PairwiseComparisonEvaluator (2-3 days)
**Priority:** HIGHEST
**Why:** Essential for A/B testing and model comparison

**Features:**
- Compare two outputs
- Aspect-level comparison
- Winner determination
- Confidence scoring

**Example:**
```python
comparison = await compare(
    output_a="GPT-4 response",
    output_b="Claude response",
    criteria="helpfulness, accuracy, clarity",
    reference="User question"
)
print(f"Winner: {comparison.winner}")
```

**Deliverables:**
- [ ] PairwiseComparisonEvaluator class
- [ ] ComparisonResult model
- [ ] compare() API function
- [ ] Aspect-level scoring
- [ ] Tests (>80% coverage)
- [ ] Example code
- [ ] Documentation

#### üü° Multi-Evaluator Error Handling (1 day)
**Priority:** MEDIUM

**Features:**
- Partial results when one evaluator fails
- Clear error messages
- Error tracking in result
- Graceful degradation

**Deliverables:**
- [ ] Partial result support
- [ ] Error field in EvaluationResult
- [ ] Error handling tests
- [ ] Documentation

#### üü° Evaluator Registry & Validation (1 day)
**Priority:** MEDIUM

**Features:**
- Registry of available evaluators
- Validate evaluator names early
- Better error messages
- Type-safe evaluator names (Literal)

**Deliverables:**
- [ ] AVAILABLE_EVALUATORS registry
- [ ] Validation in evaluate()
- [ ] Type hints (Literal["semantic", "custom_criteria", ...])
- [ ] Tests

#### üü¢ Documentation & Examples (5-7 days)
**Priority:** HIGH

**Deliverables:**
- [ ] 10-15 usage examples
  - Basic evaluation
  - Custom criteria
  - Pairwise comparison
  - Multiple evaluators
  - Middleware usage
  - Cost tracking
  - Error handling
  - Batch (manual loop)
  - Provider switching
  - Advanced configuration
- [ ] API reference documentation
- [ ] Integration guides
- [ ] Performance best practices
- [ ] Troubleshooting guide

### Success Criteria
- ‚úÖ CustomCriteriaEvaluator working with tests
- ‚úÖ PairwiseComparisonEvaluator working with compare() API
- ‚úÖ 10+ examples covering key use cases
- ‚úÖ Comprehensive API documentation
- ‚úÖ Error handling tested and robust

### Risks
- **Scope creep:** Focus on must-haves only
- **Documentation time:** May take longer than estimated

### Dependencies
- None (can start immediately)

---

## Phase 3: Semantic Comparison ‚è≥ NEXT

**Duration:** 2 weeks (Dec 13-26, 2025)
**Status:** ‚è≥ Planned
**Goal:** Milvus integration for vector-based semantic comparison

### Objectives
1. Set up Milvus vector database
2. Implement embedding generation
3. Build vector similarity scoring
4. Hybrid evaluation (LLM + vector similarity)

### Deliverables

#### Vector Storage
- [ ] Milvus client (core/milvus_client.py)
- [ ] Embedding generation (core/embeddings.py)
  - OpenAI embeddings
  - Sentence transformers
  - Custom embedding models
- [ ] Vector storage schema design
- [ ] Collection management

#### Enhanced SemanticEvaluator
- [ ] Hybrid mode (LLM + vector similarity)
- [ ] Vector-only mode (faster, cheaper)
- [ ] Embedding caching
- [ ] Similarity threshold configuration

#### Infrastructure
- [ ] Milvus connection pooling
- [ ] Embedding cache
- [ ] Vector search optimization

### Success Criteria
- ‚úÖ Milvus running locally
- ‚úÖ Embeddings generated and stored
- ‚úÖ Vector similarity scoring works
- ‚úÖ Hybrid evaluation faster than LLM-only
- ‚úÖ Tests for Milvus integration

### Risks
- **Milvus complexity:** May require Docker setup
- **Performance:** Embedding generation adds latency
- **Storage:** Vector storage size management

### Dependencies
- Phase 2.5 complete (can start in parallel)

---

## Phase 4: Storage & Batch Operations ‚è≥ PLANNED

**Duration:** 2 weeks (Dec 27 - Jan 9, 2026)
**Status:** ‚è≥ Planned
**Goal:** Storage backends and batch processing

### Objectives
1. Implement storage backends
2. Add batch evaluation
3. Result serialization and caching

### Deliverables

#### Storage Backends
- [ ] StorageBackend protocol (done in Phase 1, implement now)
- [ ] MemoryStorage (in-memory LRU cache)
- [ ] FileStorage (JSON persistence)
- [ ] RedisStorage (distributed caching)
- [ ] Result serialization/deserialization

#### Batch Operations
- [ ] batch_evaluate() API function
- [ ] Parallel processing of multiple outputs
- [ ] Progress tracking
- [ ] Error handling for partial failures
- [ ] Result aggregation

#### Optimizations
- [ ] Connection pooling (already done, optimize)
- [ ] Request batching
- [ ] Result streaming (async iterator)

### Success Criteria
- ‚úÖ 3 storage backends working
- ‚úÖ batch_evaluate() handles 100+ items
- ‚úÖ Parallel processing faster than sequential
- ‚úÖ Partial failure handling tested
- ‚úÖ Storage integration tests

### Risks
- **Redis dependency:** Adds infrastructure requirement
- **Batch complexity:** Error handling edge cases

### Dependencies
- Phase 2.5 complete
- Phase 3 optional (can parallelize)

---

## Phase 5: Core Evaluators ‚è≥ PLANNED

**Duration:** 6 weeks (Jan 10 - Feb 20, 2026)
**Status:** ‚è≥ Planned
**Goal:** Build 6-8 production evaluators

### Objectives
1. Implement core evaluators based on market demand
2. Achieve competitive evaluator coverage
3. Ensure high quality and reliability

### Deliverables

#### Week 1-2: Factuality & Relevance
- [ ] **FactualityEvaluator** (1 week)
  - LLM-based fact checking
  - Claims extraction
  - Verification against reference
  - Optional web search integration
  - Output: score, factual_errors, verified_claims, uncertain_claims

- [ ] **RelevanceEvaluator** (1 week)
  - Query-output alignment assessment
  - Addressed vs. missed aspects
  - Off-topic content detection
  - Output: score, addressed_points, missing_points, irrelevant_content

#### Week 3: Toxicity
- [ ] **ToxicityEvaluator** (1 week)
  - Perspective API integration (primary)
  - LLM-based secondary check
  - Category detection (hate speech, harassment, etc.)
  - Severity levels
  - Output: score, categories, flagged_content, severity

#### Week 4: Groundedness (RAG)
- [ ] **GroundednessEvaluator** (1 week)
  - Source attribution checking
  - Unsupported claims detection
  - Citation mapping
  - Output: score, unsupported_claims, citations, attribution_rate

#### Week 5: Consistency
- [ ] **ConsistencyEvaluator** (1 week)
  - Internal consistency checking
  - Contradiction detection
  - Temporal consistency
  - Output: score, contradictions, logical_issues

#### Week 6: Context Relevance (RAG)
- [ ] **ContextRelevanceEvaluator** (1 week)
  - Retrieved context quality assessment
  - Useful vs. irrelevant chunks
  - Coverage scoring
  - Output: score, useful_chunks, irrelevant_chunks, coverage_score

### Success Criteria
- ‚úÖ 6 evaluators fully implemented
- ‚úÖ Each evaluator >80% test coverage
- ‚úÖ Documentation for each evaluator
- ‚úÖ Examples for each use case
- ‚úÖ Performance benchmarks

### Risks
- **Quality variation:** Some evaluators harder than others
- **External APIs:** Toxicity eval depends on Perspective API
- **Time:** May take longer than estimated

### Dependencies
- Phase 2.5 complete (CustomCriteria provides template)
- Phase 3 optional
- Phase 4 optional

---

## Phase 6: Evaluation Quality Assurance ‚è≥ PLANNED

**Duration:** 2-3 weeks (Feb 21 - Mar 13, 2026)
**Status:** ‚è≥ Planned
**Goal:** Tools to validate evaluator quality

**Differentiator:** Only framework with built-in quality assurance

### Objectives
1. Build evaluator validation tools
2. Create calibration metrics
3. Measure agreement with human judgment
4. Test consistency and robustness

### Deliverables

#### Validation Tools
- [ ] validate_evaluator() - Agreement with human labels
  - Kendall tau correlation
  - Spearman correlation
  - Accuracy metrics
  - Bias analysis

- [ ] check_evaluator_agreement() - Inter-evaluator agreement
  - Compare multiple evaluators
  - Identify discrepancies
  - Consensus scoring

- [ ] test_evaluator_consistency() - Consistency testing
  - Same input multiple times
  - Measure variance
  - Confidence calibration

- [ ] adversarial_test() - Robustness testing
  - Typos
  - Reordering
  - Paraphrasing
  - Perturbation resilience

- [ ] calibrate_evaluator() - Calibration analysis
  - Confidence accuracy
  - Over/under-confidence detection
  - Calibration curves

#### Documentation
- [ ] Quality assurance guide
- [ ] Validation best practices
- [ ] Calibration datasets
- [ ] Benchmark results

### Success Criteria
- ‚úÖ 5 validation tools implemented
- ‚úÖ Human agreement >0.75 for core evaluators
- ‚úÖ Consistency variance <0.1
- ‚úÖ Calibration documented
- ‚úÖ Published benchmarks

### Risks
- **Human labeling:** Need labeled datasets
- **Metrics complexity:** Statistical rigor required

### Dependencies
- Phase 5 complete (need evaluators to validate)

---

## Phase 7: Streaming Support ‚è≥ FUTURE

**Duration:** 1 week (Mar 14-20, 2026)
**Status:** ‚è≥ Future
**Goal:** ByteWax integration for streaming data

### Objectives
1. Design streaming adapter interface
2. Implement ByteWax adapter
3. Create streaming examples

### Deliverables

#### ByteWax Integration
- [ ] StreamingAdapter protocol
- [ ] ByteWaxAdapter implementation
  - Input operators
  - Output operators
  - State management
- [ ] Streaming examples
  - Kafka ‚Üí Arbiter ‚Üí Sink
  - File stream processing
  - Real-time evaluation

### Success Criteria
- ‚úÖ ByteWax adapter works
- ‚úÖ Handles 1000+ items/sec
- ‚úÖ State management tested
- ‚úÖ Examples provided

### Risks
- **ByteWax complexity:** Learning curve
- **Performance:** May require optimization

### Dependencies
- Phase 4 complete (batch operations foundation)

---

## Phase 8: Testing & Documentation ‚è≥ FUTURE

**Duration:** 2 weeks (Mar 21 - Apr 3, 2026)
**Status:** ‚è≥ Future
**Goal:** Comprehensive testing and documentation

### Objectives
1. Achieve >80% test coverage
2. Complete API documentation
3. Write user guides
4. Create architecture docs

### Deliverables

#### Testing
- [ ] Unit tests (all modules >80% coverage)
- [ ] Integration tests (end-to-end flows)
- [ ] Performance tests (benchmarks)
- [ ] Load tests (concurrent processing)

#### Documentation
- [ ] API reference (all public functions)
- [ ] User guides
  - Installation
  - Quick start
  - Evaluator guide
  - Configuration
  - Advanced patterns
- [ ] Architecture documentation
  - Design decisions
  - Component interactions
  - Extension points
- [ ] Examples (15+ comprehensive examples)

### Success Criteria
- ‚úÖ >80% test coverage achieved
- ‚úÖ All public APIs documented
- ‚úÖ User guides complete
- ‚úÖ Architecture documented

### Risks
- **Time:** Documentation takes longer than expected

### Dependencies
- Phases 1-7 complete

---

## Phase 9: Polish & Release ‚è≥ FUTURE

**Duration:** 2 weeks (Apr 4-17, 2026)
**Status:** ‚è≥ Future
**Goal:** Prepare for v1.0 release

### Objectives
1. Set up CI/CD
2. Package for PyPI
3. Create documentation site
4. Announce release

### Deliverables

#### CI/CD
- [ ] GitHub Actions workflows
  - Test automation
  - Type checking
  - Linting
  - Coverage reporting
- [ ] Pre-commit hooks
  - Black formatting
  - Ruff linting
  - Mypy type checking

#### Package
- [ ] Build wheels
- [ ] Test installation
- [ ] PyPI upload
- [ ] Version tagging

#### Documentation Site
- [ ] MkDocs configuration
- [ ] Deploy to GitHub Pages
- [ ] Custom domain (optional)

#### Launch
- [ ] Blog post
- [ ] Reddit/HN announcement
- [ ] Twitter launch
- [ ] Documentation push

### Success Criteria
- ‚úÖ CI/CD running
- ‚úÖ Package on PyPI
- ‚úÖ Docs site live
- ‚úÖ Launch announced

### Risks
- **PyPI issues:** First-time package upload
- **Documentation site:** May need custom theme

### Dependencies
- All phases complete

---

## Resource Requirements

### Development Time

| Phase | Duration | FTE |
|-------|----------|-----|
| Phase 1-2 | 3 weeks | 1 |
| Phase 2.5 | 2-3 weeks | 1 |
| Phase 3 | 2 weeks | 1 |
| Phase 4 | 2 weeks | 1 |
| Phase 5 | 6 weeks | 1 |
| Phase 6 | 2-3 weeks | 1 |
| Phase 7 | 1 week | 0.5 |
| Phase 8 | 2 weeks | 1 |
| Phase 9 | 2 weeks | 0.5 |
| **Total** | **~7 months** | **1 FTE** |

### Infrastructure

**Development:**
- GitHub repository (free)
- Local development environment

**Testing:**
- LLM API credits ($100-500/month)
  - OpenAI GPT-4o-mini for testing
  - Multiple providers for validation
- Milvus (local Docker or cloud)
  - Vector storage
  - ~10GB for development

**Production (Post-Release):**
- Documentation hosting (GitHub Pages - free)
- Optional: Redis for storage backend
- Optional: ByteWax for streaming

### Estimated Costs

**Development Phase (7 months):**
- LLM API credits: $500-2000
- Infrastructure: $0-500 (if using cloud Milvus/Redis)
- **Total:** $500-2500

**Post-Release (Monthly):**
- Hosting: $0 (GitHub Pages)
- LLM credits (testing): $50-200
- Infrastructure (optional): $0-100

---

## Risk Management

### High-Priority Risks

#### 1. Evaluator Coverage Gap üî¥
**Risk:** Only semantic evaluator blocks adoption
**Impact:** HIGH
**Mitigation:** Phase 2.5 adds CustomCriteria (covers long tail)
**Status:** Being addressed

#### 2. Documentation Gap üü°
**Risk:** Poor docs slow adoption
**Impact:** MEDIUM
**Mitigation:** Phase 2.5 adds 10+ examples
**Status:** Being addressed

#### 3. PydanticAI Breaking Changes üü°
**Risk:** New library, may have breaking changes
**Impact:** MEDIUM
**Mitigation:** Abstract behind interfaces
**Status:** Monitored

### Medium-Priority Risks

#### 4. LLM Cost at Scale üü°
**Risk:** Expensive with GPT-4
**Impact:** MEDIUM
**Mitigation:** Caching, cheaper models, batch ops
**Status:** Mitigated (caching exists)

#### 5. Competition üü°
**Risk:** Established players have network effects
**Impact:** MEDIUM
**Mitigation:** Focus on observability + production
**Status:** Differentiation clear

### Low-Priority Risks

#### 6. Milvus Complexity üü¢
**Risk:** Vector DB adds complexity
**Impact:** LOW
**Mitigation:** Make optional, good docs
**Status:** Contained to Phase 3

---

## Decision Log

### Major Architectural Decisions

| Date | Decision | Rationale | Trade-offs |
|------|----------|-----------|------------|
| 2025-11-01 | Use PydanticAI | Structured outputs, type safety | Dependency on new library |
| 2025-11-01 | Provider-agnostic | Future-proof, flexibility | More complexity than OpenAI-only |
| 2025-11-15 | Template method pattern | Consistency, automatic tracking | Less flexibility for unusual evaluators |
| 2025-11-15 | Automatic interaction tracking | Complete observability | Slight performance overhead |
| 2025-11-15 | LLM-as-judge | Nuanced evaluation | Cost and latency |

### Implementation Decisions

| Date | Decision | Rationale |
|------|----------|-----------|
| 2025-11-15 | Fix token counting in Phase 2 | Critical for cost tracking |
| 2025-11-15 | Integrate middleware in Phase 2 | Core feature, needed immediately |
| 2025-11-22 | Add Phase 2.5 | Address critical gaps before Phase 3 |
| 2025-11-22 | CustomCriteria highest priority | Multiplies usefulness immediately |

---

## Success Metrics & Tracking

### Technical Metrics

| Metric | Target | Current | Phase |
|--------|--------|---------|-------|
| Evaluator Count | 8+ | 1 | Phase 5 |
| Test Coverage | >80% | TBD | Phase 8 |
| p95 Latency | <200ms | TBD | Phase 8 |
| Human Agreement | >0.85 | TBD | Phase 6 |

### Adoption Metrics

| Metric | Target | Deadline | Status |
|--------|--------|----------|--------|
| GitHub Stars | 100+ | Month 3 (Feb 2026) | 0 |
| Production Users | 10+ | Month 6 (May 2026) | 0 |
| Community Evaluators | 5+ | Month 9 (Aug 2026) | 0 |
| Framework Integrations | 2+ | Month 6 (May 2026) | 0 |

### Quality Metrics

| Metric | Target | Status |
|--------|--------|--------|
| Documentation Coverage | 100% public APIs | TBD |
| Performance Benchmarks | Published | TBD |
| Calibration Data | Available | TBD |
| Community Engagement | Active (issues, PRs) | TBD |

---

## Next Steps

### Immediate (This Week)
1. ‚úÖ Start Phase 2.5
2. üöß Implement CustomCriteriaEvaluator
3. ‚è≥ Design PairwiseComparisonEvaluator

### Short-Term (Next Month)
1. Complete Phase 2.5 (all tasks)
2. Start Phase 3 (Milvus integration)
3. Build 3 core evaluators (Factuality, Relevance, Toxicity)

### Medium-Term (Months 2-3)
1. Complete Phase 3-4
2. Complete Phase 5 (all evaluators)
3. Start Phase 6 (quality assurance)

### Long-Term (Months 4-7)
1. Complete Phase 6-9
2. Launch v1.0
3. Build community

---

## Appendix

### Related Documents
- **DESIGN_SPEC.md** - Vision and architecture
- **AGENTS.md** - How to work with this repo
- **PROJECT_TODO.md** - Current milestone tracker
- **PHASE2_REVIEW.md** - Phase 2 assessment
- **EVALUATOR_RECOMMENDATIONS.md** - Evaluator priorities

### External References
- [PydanticAI](https://ai.pydantic.dev/)
- [Milvus](https://milvus.io/)
- [LangChain Evaluators](https://python.langchain.com/docs/guides/evaluation/)
- [OpenAI Evals](https://github.com/openai/evals)

### Version History
- **v1.0** (2025-11-12) - Initial project plan

---

**End of Project Plan**
